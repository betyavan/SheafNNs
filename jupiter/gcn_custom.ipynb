{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9432d13c-f150-439a-b68b-02312405e83a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorboard\n",
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80c057e9-38c1-4310-a50c-13c1e2e3bfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.pygcn.utils import load_data, accuracy\n",
    "from pygcn.pygcn.models import GCN\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ba281-b8f0-4e89-bb64-fba5e09cb363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5d6513-a5d7-4c63-8b04-75115f7acad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1d919c-2a2b-47b7-9327-c3caf5d1fc18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i.betev/Desktop/SheafNN/jupiter/pygcn/pygcn/utils.py:80: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:607.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data('pygcn/data/cora/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725648ee-ef5a-456e-9d8b-9bb2d2aaa88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "device = \"cpu\"\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "model.to(device)\n",
    "features = features.to(device)\n",
    "adj = adj.to(device)\n",
    "labels = labels.to(device)\n",
    "idx_train = idx_train.to(device)\n",
    "idx_val = idx_val.to(device)\n",
    "idx_test = idx_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b408014-0ef5-4b24-ace5-3f5529b7474e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    # log\n",
    "    writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if not args.fastmode:\n",
    "    #     # Evaluate validation set performance separately,\n",
    "    #     # deactivates dropout during validation run.\n",
    "    #     model.eval()\n",
    "    #     output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b364f52-6e9a-4821-b7c7-19635d989741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9285 acc_train: 0.1714 loss_val: 1.9406 acc_val: 0.1600 time: 0.0154s\n",
      "Epoch: 0002 loss_train: 1.9160 acc_train: 0.2071 loss_val: 1.9326 acc_val: 0.1333 time: 0.0084s\n",
      "Epoch: 0003 loss_train: 1.9091 acc_train: 0.2071 loss_val: 1.9208 acc_val: 0.1500 time: 0.0052s\n",
      "Epoch: 0004 loss_train: 1.9018 acc_train: 0.2071 loss_val: 1.9024 acc_val: 0.1600 time: 0.0046s\n",
      "Epoch: 0005 loss_train: 1.8932 acc_train: 0.2000 loss_val: 1.9034 acc_val: 0.1533 time: 0.0046s\n",
      "Epoch: 0006 loss_train: 1.8658 acc_train: 0.2143 loss_val: 1.8890 acc_val: 0.1633 time: 0.0042s\n",
      "Epoch: 0007 loss_train: 1.8540 acc_train: 0.2000 loss_val: 1.8661 acc_val: 0.1600 time: 0.0042s\n",
      "Epoch: 0008 loss_train: 1.8590 acc_train: 0.2143 loss_val: 1.8683 acc_val: 0.1600 time: 0.0040s\n",
      "Epoch: 0009 loss_train: 1.8496 acc_train: 0.2071 loss_val: 1.8608 acc_val: 0.1600 time: 0.0039s\n",
      "Epoch: 0010 loss_train: 1.8249 acc_train: 0.2929 loss_val: 1.8431 acc_val: 0.2067 time: 0.0043s\n",
      "Epoch: 0011 loss_train: 1.8233 acc_train: 0.3214 loss_val: 1.8384 acc_val: 0.3133 time: 0.0039s\n",
      "Epoch: 0012 loss_train: 1.8177 acc_train: 0.3643 loss_val: 1.8453 acc_val: 0.2967 time: 0.0036s\n",
      "Epoch: 0013 loss_train: 1.8089 acc_train: 0.3429 loss_val: 1.8186 acc_val: 0.3500 time: 0.0037s\n",
      "Epoch: 0014 loss_train: 1.7967 acc_train: 0.3714 loss_val: 1.8086 acc_val: 0.3500 time: 0.0032s\n",
      "Epoch: 0015 loss_train: 1.7928 acc_train: 0.3643 loss_val: 1.8042 acc_val: 0.3667 time: 0.0031s\n",
      "Epoch: 0016 loss_train: 1.7916 acc_train: 0.3143 loss_val: 1.8038 acc_val: 0.3467 time: 0.0031s\n",
      "Epoch: 0017 loss_train: 1.7797 acc_train: 0.2857 loss_val: 1.7809 acc_val: 0.3633 time: 0.0030s\n",
      "Epoch: 0018 loss_train: 1.7520 acc_train: 0.3357 loss_val: 1.7742 acc_val: 0.3567 time: 0.0033s\n",
      "Epoch: 0019 loss_train: 1.7424 acc_train: 0.3000 loss_val: 1.7543 acc_val: 0.3467 time: 0.0032s\n",
      "Epoch: 0020 loss_train: 1.7529 acc_train: 0.3071 loss_val: 1.7549 acc_val: 0.3533 time: 0.0030s\n",
      "Epoch: 0021 loss_train: 1.7480 acc_train: 0.3000 loss_val: 1.7621 acc_val: 0.3500 time: 0.0027s\n",
      "Epoch: 0022 loss_train: 1.7380 acc_train: 0.2857 loss_val: 1.7687 acc_val: 0.3433 time: 0.0032s\n",
      "Epoch: 0023 loss_train: 1.7335 acc_train: 0.2929 loss_val: 1.7553 acc_val: 0.3500 time: 0.0030s\n",
      "Epoch: 0024 loss_train: 1.7364 acc_train: 0.3000 loss_val: 1.7453 acc_val: 0.3500 time: 0.0033s\n",
      "Epoch: 0025 loss_train: 1.7225 acc_train: 0.3071 loss_val: 1.7352 acc_val: 0.3533 time: 0.0032s\n",
      "Epoch: 0026 loss_train: 1.7056 acc_train: 0.3071 loss_val: 1.7143 acc_val: 0.3500 time: 0.0030s\n",
      "Epoch: 0027 loss_train: 1.7142 acc_train: 0.3000 loss_val: 1.7223 acc_val: 0.3467 time: 0.0031s\n",
      "Epoch: 0028 loss_train: 1.6778 acc_train: 0.3000 loss_val: 1.6922 acc_val: 0.3567 time: 0.0029s\n",
      "Epoch: 0029 loss_train: 1.6755 acc_train: 0.3000 loss_val: 1.7100 acc_val: 0.3500 time: 0.0032s\n",
      "Epoch: 0030 loss_train: 1.6910 acc_train: 0.3000 loss_val: 1.7057 acc_val: 0.3533 time: 0.0033s\n",
      "Epoch: 0031 loss_train: 1.6595 acc_train: 0.3071 loss_val: 1.6883 acc_val: 0.3533 time: 0.0031s\n",
      "Epoch: 0032 loss_train: 1.6604 acc_train: 0.3286 loss_val: 1.6911 acc_val: 0.3500 time: 0.0031s\n",
      "Epoch: 0033 loss_train: 1.6518 acc_train: 0.3143 loss_val: 1.6928 acc_val: 0.3633 time: 0.0032s\n",
      "Epoch: 0034 loss_train: 1.6767 acc_train: 0.2929 loss_val: 1.6833 acc_val: 0.3500 time: 0.0032s\n",
      "Epoch: 0035 loss_train: 1.6344 acc_train: 0.3214 loss_val: 1.6669 acc_val: 0.3467 time: 0.0032s\n",
      "Epoch: 0036 loss_train: 1.6565 acc_train: 0.3143 loss_val: 1.6822 acc_val: 0.3500 time: 0.0032s\n",
      "Epoch: 0037 loss_train: 1.6176 acc_train: 0.3500 loss_val: 1.6730 acc_val: 0.3533 time: 0.0029s\n",
      "Epoch: 0038 loss_train: 1.5940 acc_train: 0.3571 loss_val: 1.6653 acc_val: 0.3667 time: 0.0031s\n",
      "Epoch: 0039 loss_train: 1.6222 acc_train: 0.3643 loss_val: 1.6674 acc_val: 0.3667 time: 0.0032s\n",
      "Epoch: 0040 loss_train: 1.6060 acc_train: 0.3714 loss_val: 1.6481 acc_val: 0.3700 time: 0.0029s\n",
      "Epoch: 0041 loss_train: 1.5966 acc_train: 0.4143 loss_val: 1.6323 acc_val: 0.3600 time: 0.0028s\n",
      "Epoch: 0042 loss_train: 1.5823 acc_train: 0.4071 loss_val: 1.6486 acc_val: 0.3733 time: 0.0028s\n",
      "Epoch: 0043 loss_train: 1.5781 acc_train: 0.4143 loss_val: 1.6489 acc_val: 0.3733 time: 0.0029s\n",
      "Epoch: 0044 loss_train: 1.5392 acc_train: 0.4357 loss_val: 1.6168 acc_val: 0.4000 time: 0.0027s\n",
      "Epoch: 0045 loss_train: 1.5491 acc_train: 0.3929 loss_val: 1.6373 acc_val: 0.3800 time: 0.0028s\n",
      "Epoch: 0046 loss_train: 1.5171 acc_train: 0.4500 loss_val: 1.5852 acc_val: 0.3967 time: 0.0028s\n",
      "Epoch: 0047 loss_train: 1.5049 acc_train: 0.4500 loss_val: 1.6206 acc_val: 0.3867 time: 0.0029s\n",
      "Epoch: 0048 loss_train: 1.5174 acc_train: 0.4786 loss_val: 1.6075 acc_val: 0.3967 time: 0.0033s\n",
      "Epoch: 0049 loss_train: 1.4885 acc_train: 0.4857 loss_val: 1.5708 acc_val: 0.4300 time: 0.0029s\n",
      "Epoch: 0050 loss_train: 1.4875 acc_train: 0.4500 loss_val: 1.5705 acc_val: 0.4233 time: 0.0031s\n",
      "Epoch: 0051 loss_train: 1.5019 acc_train: 0.4786 loss_val: 1.5820 acc_val: 0.4200 time: 0.0031s\n",
      "Epoch: 0052 loss_train: 1.4099 acc_train: 0.5357 loss_val: 1.5279 acc_val: 0.4300 time: 0.0031s\n",
      "Epoch: 0053 loss_train: 1.4344 acc_train: 0.5143 loss_val: 1.5346 acc_val: 0.4700 time: 0.0031s\n",
      "Epoch: 0054 loss_train: 1.4251 acc_train: 0.5000 loss_val: 1.5229 acc_val: 0.4333 time: 0.0028s\n",
      "Epoch: 0055 loss_train: 1.4159 acc_train: 0.5500 loss_val: 1.5451 acc_val: 0.4633 time: 0.0028s\n",
      "Epoch: 0056 loss_train: 1.4329 acc_train: 0.5071 loss_val: 1.5255 acc_val: 0.4733 time: 0.0032s\n",
      "Epoch: 0057 loss_train: 1.3814 acc_train: 0.5714 loss_val: 1.4960 acc_val: 0.4800 time: 0.0030s\n",
      "Epoch: 0058 loss_train: 1.4053 acc_train: 0.5143 loss_val: 1.5059 acc_val: 0.5033 time: 0.0028s\n",
      "Epoch: 0059 loss_train: 1.3578 acc_train: 0.5571 loss_val: 1.4614 acc_val: 0.4867 time: 0.0029s\n",
      "Epoch: 0060 loss_train: 1.3058 acc_train: 0.6500 loss_val: 1.4740 acc_val: 0.5333 time: 0.0029s\n",
      "Epoch: 0061 loss_train: 1.3316 acc_train: 0.6071 loss_val: 1.4854 acc_val: 0.5033 time: 0.0030s\n",
      "Epoch: 0062 loss_train: 1.3664 acc_train: 0.5786 loss_val: 1.4772 acc_val: 0.5333 time: 0.0029s\n",
      "Epoch: 0063 loss_train: 1.3207 acc_train: 0.6000 loss_val: 1.4499 acc_val: 0.5433 time: 0.0027s\n",
      "Epoch: 0064 loss_train: 1.2846 acc_train: 0.6857 loss_val: 1.4463 acc_val: 0.5400 time: 0.0027s\n",
      "Epoch: 0065 loss_train: 1.2852 acc_train: 0.6286 loss_val: 1.4526 acc_val: 0.5600 time: 0.0028s\n",
      "Epoch: 0066 loss_train: 1.2795 acc_train: 0.6643 loss_val: 1.4271 acc_val: 0.5733 time: 0.0029s\n",
      "Epoch: 0067 loss_train: 1.2710 acc_train: 0.7000 loss_val: 1.4286 acc_val: 0.5867 time: 0.0028s\n",
      "Epoch: 0068 loss_train: 1.2733 acc_train: 0.6071 loss_val: 1.4206 acc_val: 0.5533 time: 0.0028s\n",
      "Epoch: 0069 loss_train: 1.2674 acc_train: 0.6071 loss_val: 1.4060 acc_val: 0.5833 time: 0.0030s\n",
      "Epoch: 0070 loss_train: 1.1966 acc_train: 0.7071 loss_val: 1.3528 acc_val: 0.5833 time: 0.0038s\n",
      "Epoch: 0071 loss_train: 1.2126 acc_train: 0.6714 loss_val: 1.3723 acc_val: 0.5767 time: 0.0036s\n",
      "Epoch: 0072 loss_train: 1.2299 acc_train: 0.6643 loss_val: 1.3852 acc_val: 0.5600 time: 0.0030s\n",
      "Epoch: 0073 loss_train: 1.2232 acc_train: 0.6571 loss_val: 1.3456 acc_val: 0.5800 time: 0.0033s\n",
      "Epoch: 0074 loss_train: 1.1554 acc_train: 0.6786 loss_val: 1.3311 acc_val: 0.5933 time: 0.0032s\n",
      "Epoch: 0075 loss_train: 1.1640 acc_train: 0.6857 loss_val: 1.3332 acc_val: 0.5700 time: 0.0032s\n",
      "Epoch: 0076 loss_train: 1.1705 acc_train: 0.6500 loss_val: 1.3100 acc_val: 0.6000 time: 0.0031s\n",
      "Epoch: 0077 loss_train: 1.1243 acc_train: 0.7143 loss_val: 1.2775 acc_val: 0.6367 time: 0.0032s\n",
      "Epoch: 0078 loss_train: 1.1396 acc_train: 0.7000 loss_val: 1.3311 acc_val: 0.6033 time: 0.0032s\n",
      "Epoch: 0079 loss_train: 1.1121 acc_train: 0.7500 loss_val: 1.3221 acc_val: 0.6133 time: 0.0030s\n",
      "Epoch: 0080 loss_train: 1.1287 acc_train: 0.7000 loss_val: 1.3027 acc_val: 0.6000 time: 0.0028s\n",
      "Epoch: 0081 loss_train: 1.0852 acc_train: 0.7500 loss_val: 1.2613 acc_val: 0.6433 time: 0.0027s\n",
      "Epoch: 0082 loss_train: 1.0987 acc_train: 0.7500 loss_val: 1.2770 acc_val: 0.6533 time: 0.0032s\n",
      "Epoch: 0083 loss_train: 1.0662 acc_train: 0.7429 loss_val: 1.2715 acc_val: 0.6367 time: 0.0029s\n",
      "Epoch: 0084 loss_train: 1.0697 acc_train: 0.7357 loss_val: 1.2583 acc_val: 0.6600 time: 0.0029s\n",
      "Epoch: 0085 loss_train: 0.9967 acc_train: 0.7429 loss_val: 1.2545 acc_val: 0.6300 time: 0.0027s\n",
      "Epoch: 0086 loss_train: 1.0246 acc_train: 0.7714 loss_val: 1.2412 acc_val: 0.6500 time: 0.0027s\n",
      "Epoch: 0087 loss_train: 1.0184 acc_train: 0.7714 loss_val: 1.2192 acc_val: 0.6633 time: 0.0027s\n",
      "Epoch: 0088 loss_train: 0.9877 acc_train: 0.7714 loss_val: 1.1964 acc_val: 0.6500 time: 0.0026s\n",
      "Epoch: 0089 loss_train: 1.0072 acc_train: 0.7429 loss_val: 1.2063 acc_val: 0.6400 time: 0.0027s\n",
      "Epoch: 0090 loss_train: 0.9999 acc_train: 0.8071 loss_val: 1.2440 acc_val: 0.6567 time: 0.0028s\n",
      "Epoch: 0091 loss_train: 0.9426 acc_train: 0.7500 loss_val: 1.1903 acc_val: 0.6933 time: 0.0028s\n",
      "Epoch: 0092 loss_train: 0.9400 acc_train: 0.7571 loss_val: 1.2225 acc_val: 0.6433 time: 0.0032s\n",
      "Epoch: 0093 loss_train: 0.9753 acc_train: 0.7714 loss_val: 1.1691 acc_val: 0.7067 time: 0.0029s\n",
      "Epoch: 0094 loss_train: 0.9550 acc_train: 0.7571 loss_val: 1.1660 acc_val: 0.6567 time: 0.0027s\n",
      "Epoch: 0095 loss_train: 0.9705 acc_train: 0.7857 loss_val: 1.2118 acc_val: 0.6700 time: 0.0031s\n",
      "Epoch: 0096 loss_train: 0.9086 acc_train: 0.7786 loss_val: 1.1606 acc_val: 0.6800 time: 0.0029s\n",
      "Epoch: 0097 loss_train: 0.9187 acc_train: 0.7714 loss_val: 1.1977 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0098 loss_train: 0.9244 acc_train: 0.7571 loss_val: 1.1106 acc_val: 0.6800 time: 0.0032s\n",
      "Epoch: 0099 loss_train: 0.9236 acc_train: 0.7929 loss_val: 1.1609 acc_val: 0.6700 time: 0.0029s\n",
      "Epoch: 0100 loss_train: 0.8792 acc_train: 0.7929 loss_val: 1.0969 acc_val: 0.7133 time: 0.0031s\n",
      "Epoch: 0101 loss_train: 0.9230 acc_train: 0.7357 loss_val: 1.1688 acc_val: 0.6767 time: 0.0032s\n",
      "Epoch: 0102 loss_train: 0.8662 acc_train: 0.8000 loss_val: 1.1257 acc_val: 0.6867 time: 0.0031s\n",
      "Epoch: 0103 loss_train: 0.8891 acc_train: 0.8000 loss_val: 1.0629 acc_val: 0.7167 time: 0.0031s\n",
      "Epoch: 0104 loss_train: 0.8772 acc_train: 0.7857 loss_val: 1.0794 acc_val: 0.7133 time: 0.0029s\n",
      "Epoch: 0105 loss_train: 0.8563 acc_train: 0.8214 loss_val: 1.0972 acc_val: 0.7267 time: 0.0028s\n",
      "Epoch: 0106 loss_train: 0.8690 acc_train: 0.8071 loss_val: 1.1256 acc_val: 0.6800 time: 0.0027s\n",
      "Epoch: 0107 loss_train: 0.8744 acc_train: 0.8286 loss_val: 1.1332 acc_val: 0.6767 time: 0.0027s\n",
      "Epoch: 0108 loss_train: 0.8313 acc_train: 0.8071 loss_val: 1.0866 acc_val: 0.7133 time: 0.0031s\n",
      "Epoch: 0109 loss_train: 0.8611 acc_train: 0.8071 loss_val: 1.0945 acc_val: 0.6967 time: 0.0031s\n",
      "Epoch: 0110 loss_train: 0.8521 acc_train: 0.8071 loss_val: 1.0658 acc_val: 0.7133 time: 0.0030s\n",
      "Epoch: 0111 loss_train: 0.8352 acc_train: 0.8071 loss_val: 1.0879 acc_val: 0.7000 time: 0.0027s\n",
      "Epoch: 0112 loss_train: 0.8401 acc_train: 0.8214 loss_val: 1.0653 acc_val: 0.7133 time: 0.0032s\n",
      "Epoch: 0113 loss_train: 0.7763 acc_train: 0.8357 loss_val: 1.0389 acc_val: 0.7267 time: 0.0033s\n",
      "Epoch: 0114 loss_train: 0.8071 acc_train: 0.8214 loss_val: 1.0782 acc_val: 0.6933 time: 0.0028s\n",
      "Epoch: 0115 loss_train: 0.7807 acc_train: 0.8357 loss_val: 1.0404 acc_val: 0.7400 time: 0.0028s\n",
      "Epoch: 0116 loss_train: 0.7922 acc_train: 0.8429 loss_val: 1.0812 acc_val: 0.7067 time: 0.0033s\n",
      "Epoch: 0117 loss_train: 0.7841 acc_train: 0.8286 loss_val: 1.0143 acc_val: 0.7467 time: 0.0030s\n",
      "Epoch: 0118 loss_train: 0.7505 acc_train: 0.8357 loss_val: 1.0512 acc_val: 0.7267 time: 0.0028s\n",
      "Epoch: 0119 loss_train: 0.7601 acc_train: 0.8143 loss_val: 1.0034 acc_val: 0.7067 time: 0.0032s\n",
      "Epoch: 0120 loss_train: 0.7424 acc_train: 0.8500 loss_val: 1.0409 acc_val: 0.7233 time: 0.0030s\n",
      "Epoch: 0121 loss_train: 0.7714 acc_train: 0.8429 loss_val: 1.0612 acc_val: 0.7433 time: 0.0028s\n",
      "Epoch: 0122 loss_train: 0.6870 acc_train: 0.8643 loss_val: 1.0338 acc_val: 0.7300 time: 0.0030s\n",
      "Epoch: 0123 loss_train: 0.7365 acc_train: 0.8786 loss_val: 1.0535 acc_val: 0.7500 time: 0.0030s\n",
      "Epoch: 0124 loss_train: 0.7058 acc_train: 0.8643 loss_val: 1.0500 acc_val: 0.7100 time: 0.0027s\n",
      "Epoch: 0125 loss_train: 0.7113 acc_train: 0.8714 loss_val: 1.0137 acc_val: 0.6800 time: 0.0027s\n",
      "Epoch: 0126 loss_train: 0.6779 acc_train: 0.8357 loss_val: 0.9508 acc_val: 0.7367 time: 0.0031s\n",
      "Epoch: 0127 loss_train: 0.6578 acc_train: 0.8429 loss_val: 0.9825 acc_val: 0.7533 time: 0.0028s\n",
      "Epoch: 0128 loss_train: 0.6893 acc_train: 0.8571 loss_val: 0.9687 acc_val: 0.7567 time: 0.0031s\n",
      "Epoch: 0129 loss_train: 0.7168 acc_train: 0.8500 loss_val: 0.9631 acc_val: 0.7300 time: 0.0030s\n",
      "Epoch: 0130 loss_train: 0.6661 acc_train: 0.8500 loss_val: 0.9784 acc_val: 0.7200 time: 0.0026s\n",
      "Epoch: 0131 loss_train: 0.6897 acc_train: 0.8571 loss_val: 1.0169 acc_val: 0.7133 time: 0.0030s\n",
      "Epoch: 0132 loss_train: 0.6370 acc_train: 0.8786 loss_val: 0.9660 acc_val: 0.7533 time: 0.0027s\n",
      "Epoch: 0133 loss_train: 0.6586 acc_train: 0.8500 loss_val: 0.9447 acc_val: 0.7700 time: 0.0031s\n",
      "Epoch: 0134 loss_train: 0.6532 acc_train: 0.8857 loss_val: 0.9958 acc_val: 0.7233 time: 0.0028s\n",
      "Epoch: 0135 loss_train: 0.6233 acc_train: 0.8714 loss_val: 0.9669 acc_val: 0.7367 time: 0.0030s\n",
      "Epoch: 0136 loss_train: 0.6021 acc_train: 0.9000 loss_val: 0.9279 acc_val: 0.7533 time: 0.0033s\n",
      "Epoch: 0137 loss_train: 0.6963 acc_train: 0.8143 loss_val: 0.9992 acc_val: 0.7533 time: 0.0034s\n",
      "Epoch: 0138 loss_train: 0.5890 acc_train: 0.8857 loss_val: 0.9221 acc_val: 0.7467 time: 0.0030s\n",
      "Epoch: 0139 loss_train: 0.6497 acc_train: 0.8714 loss_val: 0.9990 acc_val: 0.7267 time: 0.0027s\n",
      "Epoch: 0140 loss_train: 0.6014 acc_train: 0.9143 loss_val: 0.9312 acc_val: 0.7800 time: 0.0032s\n",
      "Epoch: 0141 loss_train: 0.6169 acc_train: 0.8857 loss_val: 0.9129 acc_val: 0.7733 time: 0.0029s\n",
      "Epoch: 0142 loss_train: 0.6118 acc_train: 0.8857 loss_val: 0.9461 acc_val: 0.7267 time: 0.0027s\n",
      "Epoch: 0143 loss_train: 0.5819 acc_train: 0.9000 loss_val: 0.9020 acc_val: 0.7633 time: 0.0026s\n",
      "Epoch: 0144 loss_train: 0.6050 acc_train: 0.9000 loss_val: 0.9694 acc_val: 0.7233 time: 0.0028s\n",
      "Epoch: 0145 loss_train: 0.6006 acc_train: 0.8714 loss_val: 0.9901 acc_val: 0.7200 time: 0.0027s\n",
      "Epoch: 0146 loss_train: 0.6770 acc_train: 0.8429 loss_val: 0.9318 acc_val: 0.7400 time: 0.0027s\n",
      "Epoch: 0147 loss_train: 0.5693 acc_train: 0.9071 loss_val: 0.8729 acc_val: 0.7533 time: 0.0026s\n",
      "Epoch: 0148 loss_train: 0.6089 acc_train: 0.8571 loss_val: 0.8903 acc_val: 0.7567 time: 0.0030s\n",
      "Epoch: 0149 loss_train: 0.5857 acc_train: 0.8571 loss_val: 0.8808 acc_val: 0.7700 time: 0.0027s\n",
      "Epoch: 0150 loss_train: 0.5414 acc_train: 0.9143 loss_val: 0.8749 acc_val: 0.7700 time: 0.0027s\n",
      "Epoch: 0151 loss_train: 0.5933 acc_train: 0.8643 loss_val: 0.8927 acc_val: 0.7500 time: 0.0026s\n",
      "Epoch: 0152 loss_train: 0.5584 acc_train: 0.9143 loss_val: 0.8936 acc_val: 0.7600 time: 0.0026s\n",
      "Epoch: 0153 loss_train: 0.5440 acc_train: 0.9000 loss_val: 0.8815 acc_val: 0.7533 time: 0.0026s\n",
      "Epoch: 0154 loss_train: 0.5250 acc_train: 0.9214 loss_val: 0.8941 acc_val: 0.7633 time: 0.0026s\n",
      "Epoch: 0155 loss_train: 0.5330 acc_train: 0.9286 loss_val: 0.9163 acc_val: 0.7500 time: 0.0028s\n",
      "Epoch: 0156 loss_train: 0.5633 acc_train: 0.8857 loss_val: 0.9510 acc_val: 0.7667 time: 0.0030s\n",
      "Epoch: 0157 loss_train: 0.5575 acc_train: 0.9357 loss_val: 0.9173 acc_val: 0.7500 time: 0.0028s\n",
      "Epoch: 0158 loss_train: 0.5308 acc_train: 0.9214 loss_val: 0.9066 acc_val: 0.7200 time: 0.0027s\n",
      "Epoch: 0159 loss_train: 0.5332 acc_train: 0.9143 loss_val: 0.8372 acc_val: 0.7967 time: 0.0027s\n",
      "Epoch: 0160 loss_train: 0.5077 acc_train: 0.9286 loss_val: 0.8072 acc_val: 0.7633 time: 0.0026s\n",
      "Epoch: 0161 loss_train: 0.5042 acc_train: 0.9286 loss_val: 0.9443 acc_val: 0.7433 time: 0.0030s\n",
      "Epoch: 0162 loss_train: 0.5130 acc_train: 0.8786 loss_val: 0.8480 acc_val: 0.7733 time: 0.0026s\n",
      "Epoch: 0163 loss_train: 0.4764 acc_train: 0.9429 loss_val: 0.8712 acc_val: 0.7467 time: 0.0027s\n",
      "Epoch: 0164 loss_train: 0.5131 acc_train: 0.8857 loss_val: 0.8169 acc_val: 0.7633 time: 0.0028s\n",
      "Epoch: 0165 loss_train: 0.4981 acc_train: 0.9571 loss_val: 0.8965 acc_val: 0.7467 time: 0.0029s\n",
      "Epoch: 0166 loss_train: 0.4999 acc_train: 0.9143 loss_val: 0.8741 acc_val: 0.7533 time: 0.0026s\n",
      "Epoch: 0167 loss_train: 0.5370 acc_train: 0.9429 loss_val: 0.9291 acc_val: 0.7400 time: 0.0027s\n",
      "Epoch: 0168 loss_train: 0.5467 acc_train: 0.9000 loss_val: 0.8824 acc_val: 0.7567 time: 0.0027s\n",
      "Epoch: 0169 loss_train: 0.4947 acc_train: 0.9286 loss_val: 0.8301 acc_val: 0.7633 time: 0.0030s\n",
      "Epoch: 0170 loss_train: 0.4696 acc_train: 0.9286 loss_val: 0.8662 acc_val: 0.7600 time: 0.0027s\n",
      "Epoch: 0171 loss_train: 0.4870 acc_train: 0.9143 loss_val: 0.8471 acc_val: 0.7700 time: 0.0027s\n",
      "Epoch: 0172 loss_train: 0.5132 acc_train: 0.9000 loss_val: 0.8462 acc_val: 0.7733 time: 0.0026s\n",
      "Epoch: 0173 loss_train: 0.4766 acc_train: 0.9071 loss_val: 0.8737 acc_val: 0.7533 time: 0.0027s\n",
      "Epoch: 0174 loss_train: 0.4953 acc_train: 0.9143 loss_val: 0.8794 acc_val: 0.7533 time: 0.0027s\n",
      "Epoch: 0175 loss_train: 0.4967 acc_train: 0.9071 loss_val: 0.8925 acc_val: 0.7767 time: 0.0027s\n",
      "Epoch: 0176 loss_train: 0.4970 acc_train: 0.9357 loss_val: 0.7979 acc_val: 0.8267 time: 0.0027s\n",
      "Epoch: 0177 loss_train: 0.4414 acc_train: 0.9357 loss_val: 0.8220 acc_val: 0.7767 time: 0.0031s\n",
      "Epoch: 0178 loss_train: 0.4368 acc_train: 0.9357 loss_val: 0.7710 acc_val: 0.8033 time: 0.0027s\n",
      "Epoch: 0179 loss_train: 0.4617 acc_train: 0.9286 loss_val: 0.8421 acc_val: 0.7533 time: 0.0026s\n",
      "Epoch: 0180 loss_train: 0.4520 acc_train: 0.9214 loss_val: 0.8344 acc_val: 0.7800 time: 0.0028s\n",
      "Epoch: 0181 loss_train: 0.5176 acc_train: 0.9000 loss_val: 0.8682 acc_val: 0.7667 time: 0.0032s\n",
      "Epoch: 0182 loss_train: 0.4510 acc_train: 0.9429 loss_val: 0.8015 acc_val: 0.7633 time: 0.0028s\n",
      "Epoch: 0183 loss_train: 0.4416 acc_train: 0.9571 loss_val: 0.8194 acc_val: 0.7567 time: 0.0034s\n",
      "Epoch: 0184 loss_train: 0.4750 acc_train: 0.9071 loss_val: 0.8402 acc_val: 0.7500 time: 0.0031s\n",
      "Epoch: 0185 loss_train: 0.4612 acc_train: 0.9571 loss_val: 0.8629 acc_val: 0.7600 time: 0.0028s\n",
      "Epoch: 0186 loss_train: 0.4390 acc_train: 0.9571 loss_val: 0.8081 acc_val: 0.7800 time: 0.0027s\n",
      "Epoch: 0187 loss_train: 0.4788 acc_train: 0.9071 loss_val: 0.8107 acc_val: 0.7833 time: 0.0028s\n",
      "Epoch: 0188 loss_train: 0.4906 acc_train: 0.8929 loss_val: 0.8655 acc_val: 0.7500 time: 0.0028s\n",
      "Epoch: 0189 loss_train: 0.4641 acc_train: 0.9429 loss_val: 0.8336 acc_val: 0.7500 time: 0.0032s\n",
      "Epoch: 0190 loss_train: 0.4780 acc_train: 0.9214 loss_val: 0.8510 acc_val: 0.7767 time: 0.0028s\n",
      "Epoch: 0191 loss_train: 0.4469 acc_train: 0.9143 loss_val: 0.8073 acc_val: 0.7833 time: 0.0029s\n",
      "Epoch: 0192 loss_train: 0.4827 acc_train: 0.9143 loss_val: 0.8129 acc_val: 0.7767 time: 0.0029s\n",
      "Epoch: 0193 loss_train: 0.4891 acc_train: 0.9071 loss_val: 0.8274 acc_val: 0.7733 time: 0.0028s\n",
      "Epoch: 0194 loss_train: 0.4221 acc_train: 0.9214 loss_val: 0.8355 acc_val: 0.7700 time: 0.0029s\n",
      "Epoch: 0195 loss_train: 0.4157 acc_train: 0.9571 loss_val: 0.8352 acc_val: 0.7367 time: 0.0031s\n",
      "Epoch: 0196 loss_train: 0.4221 acc_train: 0.9500 loss_val: 0.8541 acc_val: 0.7467 time: 0.0028s\n",
      "Epoch: 0197 loss_train: 0.4570 acc_train: 0.9214 loss_val: 0.8191 acc_val: 0.7633 time: 0.0029s\n",
      "Epoch: 0198 loss_train: 0.4540 acc_train: 0.9286 loss_val: 0.8528 acc_val: 0.7200 time: 0.0027s\n",
      "Epoch: 0199 loss_train: 0.4522 acc_train: 0.9143 loss_val: 0.8097 acc_val: 0.7667 time: 0.0029s\n",
      "Epoch: 0200 loss_train: 0.4637 acc_train: 0.9071 loss_val: 0.8502 acc_val: 0.7667 time: 0.0029s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.6220s\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "    \n",
    "writer.flush()\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b3e335-e3c8-4a6c-ab3e-332aa2abbde7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32a969e-0981-4ae5-b29c-75ceadcaab92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc4139-767a-4435-9a65-fd50019190a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed668cf-0a04-4f6c-a7e8-ec415f229a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
